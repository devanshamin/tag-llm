dataset: pubmed
feature_type: explanation
cache_dir: .cache
seed: 42

lm_encoder:
  model_name_or_path: avsolatorio/GIST-Embedding-v0
  model_library: sentence_transformer
  sentence_transformer_encoder_args:
    batch_size: 50
    show_progress_bar: True
    precision: float32
  cache_dir: ${cache_dir}

llm_connector:
  dataset_name: ${dataset}
  cache_dir: ${cache_dir}
  inference_args:
    #model: groq/mixtral-8x7b-32768 # https://litellm.vercel.app/docs/providers
    model: anthropic/claude-3-haiku-20240307
    max_tokens: 4000

gnn_model:
  conv_layer: SAGEConv # `torch_geometric.nn.conv` layer
  hidden_channels: 64
  num_layers: 4
  dropout: 0.1

gnn_trainer:
  epochs: 100
  lr: 0.0031622776601683794 # 10**-2.5
  weight_decay: 0.00001 # 10**-5

