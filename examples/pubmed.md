# [PubMed-Diabetes](https://linqs.org/datasets/#pubmed-diabetes) Dataset

## Training

<details>
<summary>Training config</summary>

```yaml
dataset: PUBMED
feature_type: TAPE
cache_dir: .cache
seed: 42

lm_encoder:
  dataset_name: ${dataset}
  feature_type: ${feature_type}
  model_name_or_path: avsolatorio/GIST-Embedding-v0
  model_library: sentence_transformer
  sentence_transformer_encoder_args:
    batch_size: 100
    show_progress_bar: True
    precision: float32
  cache_dir: ${cache_dir}

llm_online_engine:
  cache_dir: ${cache_dir}
  sampling_kwargs:
    max_tokens: 500 # LLM completion tokens
  model: huggingface/meta-llama/Meta-Llama-3-8B-Instruct

gnn_model:
  conv_layer: SAGEConv # `torch_geometric.nn.conv` layer
  hidden_channels: 64
  num_layers: 4
  dropout: 0.1

gnn_trainer:
  epochs: 500
  early_stopping_patience: 50
  lr: 0.0031622776601683794 # 10**-2.5
  weight_decay: 0.00001 # 10**-5
```
</details>

```bash
$ tag_llm_train --config=train_config.yaml --seed_runs=4
```

- The [train_config.yaml](./train_config.yaml) utilizes the online LLM engine with the model [huggingface/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).
- Predictions generated by this model for the PubMed dataset have been uploaded to [Hugging Face](https://huggingface.co/datasets/devanshamin/PubMedDiabetes-LLM-Predictions), which will be downloaded and used instead of calling the LLM during training.
- This optimization significantly accelerates the training process and demonstrates end-to-end training with tape.

## Results

- Instead of fine-tuning an LM on the PubMed dataset, the [train_config.yaml](./train_config.yaml) uses a general-purpose embedding model [avsolatorio/GIST-Embedding-v0](https://huggingface.co/avsolatorio/GIST-Embedding-v0).
- With LLM predictions, you can expect the following run time and accuracy when training the GNN for the `PubMed` dataset using the feature type `TAPE`:

```text
When the LM embeddings cache for the dataset is empty,
Feature_type        Test_accuracy
TITLE_ABSTRACT (TA)      0.908722
PREDICTION (P)           0.889959
EXPLANATION (E)          0.914807
TAPE (TAPE)              0.946501
Run time: 11 minutes and 14.59 seconds

When the LM embeddings cache for the dataset is present,
Feature_type        Test_accuracy
TITLE_ABSTRACT (TA)      0.915061
PREDICTION (P)           0.889452
EXPLANATION (E)          0.923174
TAPE (TAPE)              0.952333
Run time: 1 minute and 0.31 seconds
```

In summary,

| | Current Implementation | Author Implementation |
| -- | -- | -- |
| LLM  | `meta-llama/Meta-Llama-3-8B-Instruct` | `openai/gpt-3.5-turbo-0301` |
| LM fine-tuning | ✖ | ✔ |
| GNN layer | `SAGEConv` | `SAGEConv` |
| GNN hparams | `layers=4, hidden_dim=64, dropout=0.1` | `layers=3, hidden_dim=256, dropout=0.5` |
| Seed runs | 4 | 4 |
| Accuracy | `0.9573 ± 0.0032` | `0.9618 ± 0.0053` |
